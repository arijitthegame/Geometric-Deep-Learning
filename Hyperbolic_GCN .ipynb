{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from HGCN paper by Chami and Ying et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch \n",
    "import numpy as np \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some basic math functions\n",
    "def cosh(x, clamp=15):\n",
    "    return x.clamp(-clamp, clamp).cosh()\n",
    "\n",
    "\n",
    "def sinh(x, clamp=15):\n",
    "    return x.clamp(-clamp, clamp).sinh()\n",
    "\n",
    "\n",
    "def tanh(x, clamp=15):\n",
    "    return x.clamp(-clamp, clamp).tanh()\n",
    "\n",
    "\n",
    "def arcosh(x):\n",
    "    return Arcosh.apply(x)\n",
    "\n",
    "\n",
    "def arsinh(x):\n",
    "    return Arsinh.apply(x)\n",
    "\n",
    "\n",
    "def artanh(x):\n",
    "    return Artanh.apply(x)\n",
    "\n",
    "\n",
    "class Artanh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = x.clamp(-1 + 1e-15, 1 - 1e-15)\n",
    "        ctx.save_for_backward(x)\n",
    "        z = x.double()\n",
    "        return (torch.log_(1 + z).sub_(torch.log_(1 - z))).mul_(0.5).to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output / (1 - input ** 2)\n",
    "\n",
    "\n",
    "class Arsinh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        z = x.double()\n",
    "        return (z + torch.sqrt_(1 + z.pow(2))).clamp_min_(1e-15).log_().to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output / (1 + input ** 2) ** 0.5\n",
    "\n",
    "\n",
    "class Arcosh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = x.clamp(min=1.0 + 1e-15)\n",
    "        ctx.save_for_backward(x)\n",
    "        z = x.double()\n",
    "        return (z + torch.sqrt_(z.pow(2) - 1)).clamp_min_(1e-15).log_().to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output / (input ** 2 - 1) ** 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operations on a Lorentz model of the Hyperbolic space\n",
    "eps = {torch.float32: 1e-7, torch.float64: 1e-15}\n",
    "min_norm = 1e-15\n",
    "max_norm = 1e6\n",
    "\n",
    "def minkowski_dot(x, y, keepdim=True):\n",
    "    res = torch.sum(x * y, dim=-1) - 2 * x[..., 0] * y[..., 0]\n",
    "    if keepdim:\n",
    "        res = res.view(res.shape + (1,))\n",
    "    return res\n",
    "\n",
    "def minkowski_norm(u, keepdim=True):\n",
    "    dot = minkowski_dot(u, u, keepdim=keepdim)\n",
    "    return torch.sqrt(torch.clamp(dot, min=eps[u.dtype]))\n",
    "\n",
    "def sqdist(x, y, c):\n",
    "    K = 1. / c\n",
    "    prod = minkowski_dot(x, y)\n",
    "    theta = torch.clamp(-prod / K, min=1.0 + eps[x.dtype])\n",
    "    sqdist = K * arcosh(theta) ** 2\n",
    "        # clamp distance to avoid nans in Fermi-Dirac decoder\n",
    "    return torch.clamp(sqdist, max=50.0)\n",
    "\n",
    "def proj(x, c):\n",
    "    K = 1. / c\n",
    "    d = x.size(-1) - 1\n",
    "    y = x.narrow(-1, 1, d)\n",
    "    y_sqnorm = torch.norm(y, p=2, dim=1, keepdim=True) ** 2 \n",
    "    mask = torch.ones_like(x)\n",
    "    mask[:, 0] = 0\n",
    "    vals = torch.zeros_like(x)\n",
    "    vals[:, 0:1] = torch.sqrt(torch.clamp(K + y_sqnorm, min=eps[x.dtype]))\n",
    "    return vals + mask * x\n",
    "\n",
    "def proj_tan(u, x, c):\n",
    "    K = 1. / c\n",
    "    d = x.size(1) - 1\n",
    "    ux = torch.sum(x.narrow(-1, 1, d) * u.narrow(-1, 1, d), dim=1, keepdim=True)\n",
    "    mask = torch.ones_like(u)\n",
    "    mask[:, 0] = 0\n",
    "    vals = torch.zeros_like(u)\n",
    "    vals[:, 0:1] = ux / torch.clamp(x[:, 0:1], min=eps[x.dtype])\n",
    "    return vals + mask * u\n",
    "\n",
    "def proj_tan0(u):\n",
    "    narrowed = u.narrow(-1, 0, 1)\n",
    "    vals = torch.zeros_like(u)\n",
    "    vals[:, 0:1] = narrowed\n",
    "    return u - vals\n",
    "\n",
    "def expmap(u, x, c):\n",
    "    K = 1. / c\n",
    "    sqrtK = K ** 0.5\n",
    "    normu = minkowski_norm(u)\n",
    "    normu = torch.clamp(normu, max=max_norm)\n",
    "    theta = normu / sqrtK\n",
    "    theta = torch.clamp(theta, min=min_norm)\n",
    "    result = cosh(theta) * x + sinh(theta) * u / theta\n",
    "    return proj(result, c)\n",
    "        \n",
    "def logmap(x, y, c):\n",
    "    K = 1. / c\n",
    "    xy = torch.clamp(minkowski_dot(x, y) + K, max=-eps[x.dtype]) - K\n",
    "    u = y + xy * x * c\n",
    "    normu = minkowski_norm(u)\n",
    "    normu = torch.clamp(normu, min=min_norm)\n",
    "    dist = sqdist(x, y, c) ** 0.5\n",
    "    result = dist * u / normu\n",
    "    return proj_tan(result, x, c)\n",
    "\n",
    "def expmap0(u, c):\n",
    "    K = 1. / c\n",
    "    sqrtK = K ** 0.5\n",
    "    d = u.size(-1) - 1\n",
    "    x = u.narrow(-1, 1, d).view(-1, d)\n",
    "    x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "    x_norm = torch.clamp(x_norm, min=min_norm)\n",
    "    theta = x_norm / sqrtK\n",
    "    res = torch.ones_like(u)\n",
    "    res[:, 0:1] = sqrtK * cosh(theta)\n",
    "    res[:, 1:] = sqrtK * sinh(theta) * x / x_norm\n",
    "    return proj(res, c)\n",
    "\n",
    "def logmap0(x, c):\n",
    "    K = 1. / c\n",
    "    sqrtK = K ** 0.5\n",
    "    d = x.size(-1) - 1\n",
    "    y = x.narrow(-1, 1, d).view(-1, d)\n",
    "    y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n",
    "    y_norm = torch.clamp(y_norm, min=min_norm)\n",
    "    res = torch.zeros_like(x)\n",
    "    theta = torch.clamp(x[:, 0:1] / sqrtK, min=1.0 + eps[x.dtype])\n",
    "    res[:, 1:] = sqrtK * arcosh(theta) * y / y_norm\n",
    "    return res\n",
    "\n",
    "def mobius_add(x, y, c):\n",
    "    u = logmap0(y, c)\n",
    "    v = ptransp0(x, u, c)\n",
    "    return expmap(v, x, c)\n",
    "\n",
    "def mobius_matvec(m, x, c):\n",
    "    u = logmap0(x, c)\n",
    "    mu = u @ m.transpose(-1, -2)\n",
    "    return expmap0(mu, c)\n",
    "\n",
    "def ptransp(x, y, u, c):\n",
    "    logxy = logmap(x, y, c)\n",
    "    logyx = logmap(y, x, c)\n",
    "    sqdist = torch.clamp(sqdist(x, y, c), min=min_norm)\n",
    "    alpha = minkowski_dot(logxy, u) / sqdist\n",
    "    res = u - alpha * (logxy + logyx)\n",
    "    return proj_tan(res, y, c)\n",
    "\n",
    "def ptransp0(x, u, c):\n",
    "    K = 1. / c\n",
    "    sqrtK = K ** 0.5\n",
    "    x0 = x.narrow(-1, 0, 1)\n",
    "    d = x.size(-1) - 1\n",
    "    y = x.narrow(-1, 1, d)\n",
    "    y_norm = torch.clamp(torch.norm(y, p=2, dim=1, keepdim=True), min=min_norm)\n",
    "    y_normalized = y / y_norm\n",
    "    v = torch.ones_like(x)\n",
    "    v[:, 0:1] = - y_norm \n",
    "    v[:, 1:] = (sqrtK - x0) * y_normalized\n",
    "    alpha = torch.sum(y_normalized * u[:, 1:], dim=1, keepdim=True) / sqrtK\n",
    "    res = u - alpha * v\n",
    "    return proj_tan(res, x, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO These functions are not probably correct. Check and fix \n",
    "def _lambda_x(x, c):\n",
    "        x_sqnorm = torch.sum(x.data.pow(2), dim=-1, keepdim=True)\n",
    "        return 2 / (1. - c * x_sqnorm).clamp_min(1e-15)\n",
    "    \n",
    "def egrad2rgrad(p, dp, c):\n",
    "        lambda_p = _lambda_x(p, c)\n",
    "        dp /= lambda_p.pow(2)\n",
    "        return dp\n",
    "    \n",
    "def inner(x, c, u, v=None, keepdim=False):\n",
    "        if v is None:\n",
    "            v = u\n",
    "        lambda_x = _lambda_x(x, c)\n",
    "        return lambda_x ** 2 * (u * v).sum(dim=-1, keepdim=keepdim)\n",
    "    \n",
    "def to_poincare(x, c):\n",
    "        K = 1. / c\n",
    "        sqrtK = K ** 0.5\n",
    "        d = x.size(-1) - 1\n",
    "        return sqrtK * x.narrow(-1, 1, d) / (x[:, 0:1] + sqrtK)\n",
    "    \n",
    "#TODO Write a an inverse function of the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperbolic linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, use_bias):\n",
    "        super(HypLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.c = nn.Parameter(torch.Tensor([1.0]))\n",
    "        #self.dropout = dropout\n",
    "        self.use_bias = use_bias\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.xavier_uniform_(self.weight, gain=math.sqrt(2))\n",
    "        init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        drop_weight = F.dropout(self.weight, .4, training=self.training)\n",
    "        mv = mobius_matvec(self.weight, x, self.c)\n",
    "        res = proj(mv, self.c)\n",
    "        if self.use_bias:\n",
    "            bias = proj_tan0(self.bias.view(1, -1))\n",
    "            hyp_bias = expmap0(bias, self.c)\n",
    "            hyp_bias = proj(hyp_bias, self.c)\n",
    "            res = mobius_add(res, hyp_bias, c=self.c)\n",
    "            res = proj(res, self.c)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypAct(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperbolic activation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, act):\n",
    "        super(HypAct, self).__init__()\n",
    "\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        xt = self.act(logmap0(x, c=self.c_in))\n",
    "        xt = proj_tan0(xt)\n",
    "        return proj(expmap0(xt, c=self.c_out), c=self.c_out)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'c_in={}, c_out={}'.format(\n",
    "            self.c_in, self.c_out\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypAgg(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperbolic aggregation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c, in_features, dropout, use_att):\n",
    "        super(HypAgg, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.dropout = dropout\n",
    "        self.use_att = use_att\n",
    "        if self.use_att:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x_tangent = logmap0(x, c=self.c)\n",
    "        if self.use_att:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            support_t = torch.spmm(adj, x_tangent)\n",
    "        output = proj(expmap0(support_t, c=self.c), c=self.c)\n",
    "        return output\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'c={}'.format(self.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperbolic Graph Encoder model for node classification\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats, h1_feats, h2_feats, out_feats):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        \n",
    "        self.in_feats = in_feats\n",
    "        self.h1_feats = h1_feats\n",
    "        self.h2_feats = h2_feats\n",
    "        self.out_feats = out_feats\n",
    "        \n",
    "        \n",
    "        #self.c_out = nn.Parameter(torch.Tensor([1.0]))\n",
    "        \n",
    "        self.linear1 = HypLinear(self.in_feats+1, self.h1_feats, use_bias=True)\n",
    "        self.linear2 = HypLinear(self.h1_feats, self.h2_feats, use_bias=True)\n",
    "        \n",
    "        self.agg1 = HypAgg(self.linear1.state_dict()['c'], self.h1_feats, False, False)\n",
    "        self.activation1 = HypAct(self.linear1.state_dict()['c'], self.linear2.state_dict()['c'], nn.Tanh())\n",
    "        \n",
    "        self.agg2 = HypAgg(self.linear2.state_dict()['c'], self.h2_feats, False, False)\n",
    "        self.activation2 = HypAct(self.linear2.state_dict()['c'], self.linear2.state_dict()['c'], nn.Tanh())\n",
    "        \n",
    "        self.linear_out = nn.Linear(self.h2_feats, self.out_feats)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        #Intreprating the vectors as elements of the tangent space, i.e. R^{n+1}\n",
    "        o = torch.zeros_like(x)\n",
    "        x = torch.cat([o[:, 0:1], x], dim=1)\n",
    "        \n",
    "        #Projecting to hyperbolic coordinates\n",
    "        x_tan = proj_tan0(x)\n",
    "        x_hyp = expmap0(x_tan, self.linear1.state_dict()['c'])\n",
    "        x_hyp = proj(x_hyp, self.linear1.state_dict()['c'])\n",
    "        \n",
    "        #Encoder\n",
    "        x = self.linear1(x_hyp)\n",
    "        x = self.agg1(x, adj)\n",
    "        x = self.activation1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.agg2(x,adj)\n",
    "        x = self.activation2(x)\n",
    "        \n",
    "        #Decoder\n",
    "        h = proj_tan0(logmap0(x, self.linear2.state_dict()['c']))\n",
    "        x = self.linear_out(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model \n",
    "ge = GraphEncoder(32,7,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to figure the Riemannian Adam \n",
    "class OptimMixin(object):\n",
    "    def __init__(self, *args, stabilize=None, **kwargs):\n",
    "        self._stabilize = stabilize\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def stabilize_group(self, group):\n",
    "        pass\n",
    "\n",
    "    def stabilize(self):\n",
    "        \"\"\"Stabilize parameters if they are off-manifold due to numerical reasons\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            self.stabilize_group(group)\n",
    "\n",
    "\n",
    "def copy_or_set_(dest, source):\n",
    "    \"\"\"\n",
    "    A workaround to respect strides of :code:`dest` when copying :code:`source`\n",
    "    (https://github.com/geoopt/geoopt/issues/70)\n",
    "    Parameters\n",
    "    ----------\n",
    "    dest : torch.Tensor\n",
    "        Destination tensor where to store new data\n",
    "    source : torch.Tensor\n",
    "        Source data to put in the new tensor\n",
    "    Returns\n",
    "    -------\n",
    "    dest\n",
    "        torch.Tensor, modified inplace\n",
    "    \"\"\"\n",
    "    if dest.stride() != source.stride():\n",
    "        return dest.copy_(source)\n",
    "    else:\n",
    "        return dest.set_(source)\n",
    "\n",
    "\n",
    "class RiemannianAdam(OptimMixin, torch.optim.Adam):\n",
    "    r\"\"\"Riemannian Adam with the same API as :class:`torch.optim.Adam`\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : iterable\n",
    "        iterable of parameters to optimize or dicts defining\n",
    "        parameter groups\n",
    "    lr : float (optional)\n",
    "        learning rate (default: 1e-3)\n",
    "    betas : Tuple[float, float] (optional)\n",
    "        coefficients used for computing\n",
    "        running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    eps : float (optional)\n",
    "        term added to the denominator to improve\n",
    "        numerical stability (default: 1e-8)\n",
    "    weight_decay : float (optional)\n",
    "        weight decay (L2 penalty) (default: 0)\n",
    "    amsgrad : bool (optional)\n",
    "        whether to use the AMSGrad variant of this\n",
    "        algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "        (default: False)\n",
    "    Other Parameters\n",
    "    ----------------\n",
    "    stabilize : int\n",
    "        Stabilize parameters if they are off-manifold due to numerical\n",
    "        reasons every ``stabilize`` steps (default: ``None`` -- no stabilize)\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments\n",
    "        ---------\n",
    "        closure : callable (optional)\n",
    "            A closure that reevaluates the model\n",
    "            and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        with torch.no_grad():\n",
    "            for group in self.param_groups:\n",
    "                if \"step\" not in group:\n",
    "                    group[\"step\"] = 0\n",
    "                betas = group[\"betas\"]\n",
    "                weight_decay = group[\"weight_decay\"]\n",
    "                eps = group[\"eps\"]\n",
    "                learning_rate = group[\"lr\"]\n",
    "                amsgrad = group[\"amsgrad\"]\n",
    "                for point in group[\"params\"]:\n",
    "   #                 print(point)\n",
    "                    grad = point.grad\n",
    "                    if grad is None:\n",
    "                        continue\n",
    "#                     if isinstance(point, (ManifoldParameter)):\n",
    "#                         manifold = point.manifold\n",
    "#                         c = point.c\n",
    "#                     else:\n",
    "#                         manifold = _default_manifold\n",
    "#                         c = None\n",
    "                    if grad.is_sparse:\n",
    "                        raise RuntimeError(\n",
    "                                \"Riemannian Adam does not support sparse gradients yet (PR is welcome)\"\n",
    "                        )\n",
    "\n",
    "                    state = self.state[point]\n",
    "\n",
    "                    # State initialization\n",
    "                    if len(state) == 0:\n",
    "                        state[\"step\"] = 0\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state[\"exp_avg\"] = torch.zeros_like(point)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state[\"exp_avg_sq\"] = torch.zeros_like(point)\n",
    "                        if amsgrad:\n",
    "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                            state[\"max_exp_avg_sq\"] = torch.zeros_like(point)\n",
    "                    # make local variables for easy access\n",
    "                    exp_avg = state[\"exp_avg\"]\n",
    "                    exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "                    # actual step\n",
    "                    grad.add_(weight_decay, point)\n",
    "             #How to get the curvature ?       \n",
    "                    c = point.item()\n",
    "                    \n",
    "                    grad = egrad2rgrad(point, grad, c)\n",
    "                    exp_avg.mul_(betas[0]).add_(1 - betas[0], grad)\n",
    "                    exp_avg_sq.mul_(betas[1]).add_(\n",
    "                            1 - betas[1], inner(point, c, grad, keepdim=True)\n",
    "                    )\n",
    "                    if amsgrad:\n",
    "                        max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n",
    "                        # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                        torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                        # Use the max. for normalizing running avg. of gradient\n",
    "                        denom = max_exp_avg_sq.sqrt().add_(eps)\n",
    "                    else:\n",
    "                        denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                    group[\"step\"] += 1\n",
    "                    bias_correction1 = 1 - betas[0] ** group[\"step\"]\n",
    "                    bias_correction2 = 1 - betas[1] ** group[\"step\"]\n",
    "                    step_size = (\n",
    "                        learning_rate * bias_correction2 ** 0.5 / bias_correction1\n",
    "                    )\n",
    "\n",
    "                    # copy the state, we need it for retraction\n",
    "                    # get the direction for ascend\n",
    "                    direction = exp_avg / denom\n",
    "                    # transport the exponential averaging to the new point\n",
    "                    new_point = proj(expmap(-step_size * direction, point, c), c)\n",
    "                    exp_avg_new = manifold.ptransp(point, new_point, exp_avg, c)\n",
    "                    # use copy only for user facing point\n",
    "                    copy_or_set_(point, new_point)\n",
    "                    exp_avg.set_(exp_avg_new)\n",
    "\n",
    "                    group[\"step\"] += 1\n",
    "                if self._stabilize is not None and group[\"step\"] % self._stabilize == 0:\n",
    "                    self.stabilize_group(group)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def stabilize_group(self, group):\n",
    "        for p in group[\"params\"]:\n",
    "            if not isinstance(p):\n",
    "                 continue\n",
    "            state = self.state[p]\n",
    "            if not state:  # due to None grads\n",
    "                continue\n",
    "#             manifold = p.manifold\n",
    "#Same issue here\n",
    "            c = p.item()\n",
    "            exp_avg = state[\"exp_avg\"]\n",
    "            copy_or_set_(p, proj(p, c))\n",
    "            exp_avg.set_(proj_tan(exp_avg, u, c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
